{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Document Analysis\n",
    "\n",
    "In this assignment, we will learn how to do document classification and clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example\n",
    "\n",
    "In this example, we use [20newsgroups](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) dataset. Each sample is a document and there are totally 20 classes. \n",
    "\n",
    "### 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data target labels: [7 4 4 ... 3 1 8]\n",
      "Train data target names: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "#training samples: 11314\n",
      "#testing samples: 7532\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(\"Train data target labels: {}\".format(data_train.target))\n",
    "print(\"Train data target names: {}\".format(data_train.target_names))\n",
    "\n",
    "print('#training samples: {}'.format(len(data_train.data)))\n",
    "print('#testing samples: {}'.format(len(data_test.data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Represent documents with TF-IDF represention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "    <div>\n",
    "        <dt><abbr described-by='tf-idf-title'>TF-IDF</abbr></dt>\n",
    "        <dd role='tooltip' id='tf-idf-title'>Term Frequency-Inverse Document Frequency</dd>\n",
    "        <dd>Reflect how important a word is to a document in a collection</dd>\n",
    "        <dd>\n",
    "            $$\n",
    "                \\operatorname{TF\\_IDF}\\left(t, d, \\mathcal{D}\\right)\n",
    "                := \\operatorname{TF}\\left(t, d\\right)%\n",
    "                \\times\\operatorname{IDF}\\left(t, \\mathcal{D}\\right),\n",
    "            $$\n",
    "            where Term frequency\n",
    "            $$\n",
    "                \\operatorname{TF}\\left(t, d\\right)\n",
    "                := \\frac{\\#\\left(t\\text{ in document }d\\right)}\n",
    "                {\\#\\left(\\mathbf{words}\\text{ in document }d\\right)}\n",
    "            $$\n",
    "            measures the frequency of a word in a document,\n",
    "            and Inverse Document Frequency\n",
    "            $$\n",
    "                \\operatorname{IDF}\\left(t, \\mathcal{D}\\right)\n",
    "                := \\log\\left(\n",
    "                    \\mathbb{P}^{-1}\\!\\left\\{\\mathcal{D}\\text{ contains }t\\right\\}\n",
    "                \\right)\n",
    "            $$\n",
    "            measures the rareness of a word <strong>in all documents</strong>.\n",
    "        </dd>\n",
    "    </div>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 101631) (7532, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#TF-IDF representation for each document\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_train_vectors = vectorizer.fit_transform(data_train.data)\n",
    "data_test_vectors = vectorizer.transform(data_test.data) \n",
    "\n",
    "print(data_train_vectors.shape, data_test_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use KNN to do document classification\n",
    "\n",
    "Here, we use the cross-validation method to select $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16846385009722467\n",
      "{'n_neighbors': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "Xtr = data_train_vectors\n",
    "Ytr = data_train.target\n",
    "\n",
    "Xte = data_test_vectors\n",
    "Yte = data_test.target\n",
    "\n",
    "k_range = range(1, 5)\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "\n",
    "clf_knn =  KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "grid = GridSearchCV(clf_knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(Xtr, Ytr)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use Logistic Regression to do document classification\n",
    "Here, we also use the cross-validation method to select the regularization coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghc/Software/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/ghc/Software/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 8}\n",
      "0.6889272437599575 0.6778761181105242 0.6889272437599575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "#=====training with cross validation======\n",
    "coeff = range(1, 10)\n",
    "param_grid = dict(C=coeff)\n",
    "\n",
    "clf_lr = LogisticRegression(penalty='l2')\n",
    "\n",
    "grid = GridSearchCV(clf_lr, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(Xtr, Ytr)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "#=====testing======\n",
    "clf_lr = LogisticRegression(penalty='l2', C=grid.best_params_['C'])\n",
    "clf_lr.fit(Xtr, Ytr)\n",
    "\n",
    "y_pred = clf_lr.predict(Xte)\n",
    "\n",
    "acc = accuracy_score(Yte, y_pred)\n",
    "macro_f1 = f1_score(Yte, y_pred, average='macro')\n",
    "micro_f1 = f1_score(Yte, y_pred, average='micro')\n",
    "\n",
    "print(acc, macro_f1, micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task: Document Classification and Clustering\n",
    "\n",
    "In this task, we are going to use [BBCNews](BBC_News_Train.csv) dataset. There are 1490 articles from 5 topics, including tech, business, sport, entertainment, politics. \n",
    "\n",
    "* Task 1: Please use KNN and logistic regression to do classification, and compare their performance.\n",
    "\n",
    "* Task 2: Please use K-means to partition this dataset into 5 clusters and find the representative words in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary modules\n",
    "import pandas as pd                                         # for the dataframe\n",
    "# for TD-IDF representation\n",
    "from sklearn.model_selection import train_test_split        # for splitting the data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # creates a TF-IDF vector from data\n",
    "# for document classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATASET_FILENAME = r'BBC_News_Train.csv'                    # filename of the dataset input\n",
    "TEST_SIZE = 1.0/5.0                                         # proportion of test data\n",
    "SEED = 42                                                   # seed for sampling\n",
    "KILOBYTES_PER_BYTE = 1.0/1024.0                             # for converting bytes to KB\n",
    "MAX_RANGE = 5                                               # maximum range for KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load data and represent it with TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490 total articles\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "1192 entries\n",
      "1192 non-null Count\n",
      "dtypes: object\n",
      "memory usage: 9.3 KB\n",
      "\n",
      "1192 non-null Count\n",
      "dtypes: object\n",
      "memory usage: 9.3 KB\n",
      "\n",
      "TD-IDF training data shape: (1192, 22591)\n",
      "TD-IDF testing data shape: (298, 22591)\n"
     ]
    }
   ],
   "source": [
    "# load the BBCNews dataset\n",
    "df = pd.read_csv(DATASET_FILENAME)\n",
    "\n",
    "# split the data\n",
    "(ids_train, ids_test, X_df_train, X_df_test, y_train, y_test) = \\\n",
    "    train_test_split(df[df.columns[0]], df[df.columns[1:-1]], df[df.columns[-1]],\n",
    "                     test_size=TEST_SIZE, random_state=SEED)\n",
    "\n",
    "# reshape the X, y\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    (df.values.reshape((np.product(df.shape),))\n",
    "     for df in (X_df_train, X_df_test, y_df_train, y_df_test))\n",
    "\n",
    "#TF-IDF representation for each document\n",
    "vectorizer = TfidfVectorizer()\n",
    "M_train = vectorizer.fit_transform(X_train)\n",
    "M_test = vectorizer.transform(X_test)\n",
    "\n",
    "# get the shape and summary data\n",
    "(TOTAL_ARTICLES, _) = df.shape\n",
    "(N_ARTICLES, ) = X_train.shape\n",
    "\n",
    "# display the number of articles\n",
    "print(r\"{} total articles\".format(TOTAL_ARTICLES))\n",
    "print()\n",
    "print(type(X_train))\n",
    "print(r\"{} entries\".format(N_ARTICLES))\n",
    "print(\"{} non-null Count\".format(sum(X_train != None)))\n",
    "print(r\"dtypes: {}\".format(X_train.dtype))\n",
    "print(r\"memory usage: {:.1f} KB\".format(X_train.nbytes * KILOBYTES_PER_BYTE))\n",
    "print()\n",
    "print(\"{} non-null Count\".format(sum(y_train != None)))\n",
    "print(r\"dtypes: {}\".format(y_train.dtype))\n",
    "print(r\"memory usage: {:.1f} KB\".format(y_train.nbytes * KILOBYTES_PER_BYTE))\n",
    "print()\n",
    "print(r\"TD-IDF training data shape: {}\".format(M_train.shape))\n",
    "print(r\"TD-IDF testing data shape: {}\".format(M_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Use KNN to do document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9177701206005414\n",
      "{'n_neighbors': 4}\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(n_neighbors=range(1, (1 + MAX_RANGE)))\n",
    "\n",
    "clf_knn =  KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "grid = GridSearchCV(clf_knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(M_train, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use Logistic Regression to do document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Use K-means to do document clustering and find the 10 most representative words in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
