{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will learn the Logistic Regression model.\n",
    "\n",
    "First, please study the given example, which uses the logistic regression model for the breast cancer classification task. In this example, you will learn how to preprocess data, how to train the model, and how to evaluate the model.\n",
    "\n",
    "Based on the given example, your task is to use the logistic regression model to predict the presence of heart disease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the breast cancer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the [breast cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) dataset in sklearn. It is a binary classification dataset. Each sample has 30 numerical features, which can be found in [7.1.7](https://scikit-learn.org/stable/datasets/toy_dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 569, #features: 30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "print(\"#samples: {}, #features: {}\".format(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Split the data into two subsets and normalize the features of samples\n",
    "\n",
    "Here, we use 69 samples as the testing set and use the remained samples to train the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val: 500, test: 69\n"
     ]
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, \n",
    "                                                            test_size=0.12, \n",
    "                                                            random_state=0)\n",
    "print(\"train_val: {}, test: {}\".format(X_train_val.shape[0], X_test.shape[0]))\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_val = normalizer.fit_transform(X_train_val)\n",
    "X_test = normalizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train the logistic regression model and select the hyperparameter with cross-validation\n",
    "\n",
    "Here, we use the following logistic regression model to do cancer classification. \n",
    "\n",
    "\\begin{equation}\n",
    "\t\\min_{\\mathbf{w}} \\sum_{i=1}^{n}\\{\\log(1+\\exp(\\mathbf{w}^T\\mathbf{x}_i))-y_i\\mathbf{w}^T\\mathbf{x}_i \\} + \\lambda\\|\\mathbf{w}\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "We need to learn the model parameter $\\mathbf{w}$. However, with different hyperparameters $\\lambda$, we can get different model parameter $\\mathbf{w}$, resulting in different prediction performance. Here, we use the 5-fold cross-validation to select the hyperparameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 45  89 300 287 331 447  91  63 461 465 128 413 321 238 267 474 457  48\n",
      "  406 220 496 407 463 277  46 494 255 394 146 111 264 282 459 208  78 161\n",
      "  107 138 226 259 416 180  79  85 384 240  66  93 284 178   1 187 469  34\n",
      "   13 233 221 412 428 350 228  10 361 418 422 114 388  14 441 380   4 278\n",
      "  449 210   9  31 427  96 481 344 485 224 419 196 243 175 279 369 452 194\n",
      "  118   5 410 124 130 213 231  67 207 453]\n",
      " [232 154 456 370 426 251 392 301 317 289 448 434 354  77 199  20  84 335\n",
      "   81 204 386  32  75 100 320 433 421 129 443 402  24  57  99 152 291 261\n",
      "  283 409 318 295   8 256 212 499  52 436  54 480 466 176 346 383 371 381\n",
      "  405  95 492 185  72 359 184 262 275 281 352 156 468 102  41 349 337 169\n",
      "  374 372  22 159  16 497 326 417 343  15 487 236 348 360 393 214  69 314\n",
      "  294 227 106 285 182 319  83 398 316 347]\n",
      " [270 143  33 491 404  19 113  39   7 367 292 109 458 288 470 401 391 268\n",
      "  148 408 309 439 155 338 183 163 339 385 144  62 322 341  80 305 188 475\n",
      "  482 219 179 123 108 462  30  65 245 355 444 157 414 266 430 332  11 296\n",
      "  351 191 141 411 211   0 165 302 181 493 276 131 333 167 230   6 420 334\n",
      "  215 274 483  92  25 120 269 286  61 476  53 242 172 229 471 142 390 134\n",
      "  293 467 336 174 424  86 127 273  70  23]\n",
      " [198 358 387  82 260 203 315 139 396 324 200  29 399 495 197 132 373  94\n",
      "  432 105 299 362 488 460 490 403 486  76 217 451 216  21 325  71 365 472\n",
      "  116 312 121  58 151  42 101 425 313 376 202 171 308 342 498  55  47 254\n",
      "  150 397 160  98  59 137   2  87 173  17 119 366 423  88 415 189  60 438\n",
      "  164 437  44 323 237 375 104 363  27 258 311  74 479 272 440 125 158 400\n",
      "  353 345  12  26  18 133 303 442 193 170]\n",
      " [ 56 446 249 147 263 310 271 234   3 464 253 265 239 477 328 225 364  64\n",
      "  431 445 246 201 377  38 244 223 186 455 429 395 218 112 252 484 454 297\n",
      "  248 235 389 195 103 280  36 206 177 489 290 435  97 450  37 192 241 126\n",
      "  382 368 329  49 250  40 205 298  28  73 153  50 357 247  68  43 162 115\n",
      "  145 356 307 140 136 257 190  51  90 340 135 117 378 209 222 379 304 149\n",
      "  478 327  35 168 122 473 166 330 110 306]]\n",
      "reg_coeff: 10.0, acc: 0.968\n",
      "reg_coeff: 8.912509381337454, acc: 0.972\n",
      "reg_coeff: 7.943282347242815, acc: 0.972\n",
      "reg_coeff: 7.079457843841378, acc: 0.974\n",
      "reg_coeff: 6.3095734448019325, acc: 0.974\n",
      "reg_coeff: 5.62341325190349, acc: 0.974\n",
      "reg_coeff: 5.011872336272723, acc: 0.974\n",
      "reg_coeff: 4.466835921509632, acc: 0.976\n",
      "reg_coeff: 3.9810717055349727, acc: 0.978\n",
      "reg_coeff: 3.5481338923357546, acc: 0.978\n",
      "reg_coeff: 3.162277660168379, acc: 0.980\n",
      "reg_coeff: 2.8183829312644537, acc: 0.980\n",
      "reg_coeff: 2.5118864315095806, acc: 0.980\n",
      "reg_coeff: 2.2387211385683394, acc: 0.980\n",
      "reg_coeff: 1.9952623149688797, acc: 0.980\n",
      "reg_coeff: 1.7782794100389228, acc: 0.980\n",
      "reg_coeff: 1.5848931924611134, acc: 0.980\n",
      "reg_coeff: 1.4125375446227544, acc: 0.980\n",
      "reg_coeff: 1.2589254117941673, acc: 0.978\n",
      "reg_coeff: 1.1220184543019633, acc: 0.978\n",
      "reg_coeff: 1.0, acc: 0.978\n",
      "reg_coeff: 0.8912509381337456, acc: 0.978\n",
      "reg_coeff: 0.7943282347242815, acc: 0.978\n",
      "reg_coeff: 0.7079457843841379, acc: 0.978\n",
      "reg_coeff: 0.6309573444801932, acc: 0.976\n",
      "reg_coeff: 0.5623413251903491, acc: 0.976\n",
      "reg_coeff: 0.5011872336272724, acc: 0.976\n",
      "reg_coeff: 0.44668359215096315, acc: 0.974\n",
      "reg_coeff: 0.39810717055349726, acc: 0.974\n",
      "reg_coeff: 0.35481338923357547, acc: 0.974\n",
      "reg_coeff: 0.31622776601683794, acc: 0.972\n",
      "reg_coeff: 0.28183829312644537, acc: 0.972\n",
      "reg_coeff: 0.251188643150958, acc: 0.970\n",
      "reg_coeff: 0.22387211385683392, acc: 0.970\n",
      "reg_coeff: 0.199526231496888, acc: 0.970\n",
      "reg_coeff: 0.17782794100389226, acc: 0.970\n",
      "reg_coeff: 0.15848931924611132, acc: 0.970\n",
      "reg_coeff: 0.14125375446227545, acc: 0.970\n",
      "reg_coeff: 0.1258925411794167, acc: 0.970\n",
      "reg_coeff: 0.11220184543019636, acc: 0.970\n",
      "reg_coeff: 0.1, acc: 0.970\n",
      "best reg coeff: 3.162277660168379\n"
     ]
    }
   ],
   "source": [
    "# here we use 5-fold cross-validation\n",
    "folds = 5\n",
    "\n",
    "# get the number of samples in the training and validation set\n",
    "num_train_val = X_train_val.shape[0] \n",
    "\n",
    "# shuffle the index of samples in the train_val set\n",
    "index_of_samples = np.arange(num_train_val) \n",
    "shuffle(index_of_samples)\n",
    "\n",
    "# split the index of the train_valid set into 5 folds\n",
    "index_of_folds = index_of_samples.reshape(folds, -1)\n",
    "print(index_of_folds)\n",
    "\n",
    "# potential hyperparameters. \n",
    "reg_coeff_radius = 20\n",
    "reg_coeff_radius_dB = np.array(range((-reg_coeff_radius), (1 + reg_coeff_radius)))\n",
    "regularization_coefficient = 10**(reg_coeff_radius_dB/20)\n",
    "\n",
    "best_acc = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "for reg in regularization_coefficient:\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    sum_acc = 0.0\n",
    "    for fold in range(folds):\n",
    "        \n",
    "        index_of_folds_temp = index_of_folds.copy()\n",
    "        \n",
    "        valid_index = index_of_folds_temp[fold,:].reshape(-1) #get the index of the validation set\n",
    "        train_index = np.delete(index_of_folds_temp, fold, 0).reshape(-1) #get the index of the training set\n",
    "        \n",
    "        # training set\n",
    "        X_train = X_train_val[train_index]\n",
    "        y_train = y_train_val[train_index]\n",
    "        \n",
    "        # validation set\n",
    "        X_valid = X_train_val[valid_index]\n",
    "        y_valid = y_train_val[valid_index]\n",
    "                \n",
    "        # build the model with different hyperparameters\n",
    "        clf = LogisticRegression(penalty='l2', C=reg, solver='lbfgs')\n",
    "        \n",
    "        #train the model with the training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_pred = clf.predict(X_valid)\n",
    "        acc = accuracy_score(y_valid, y_valid_pred)\n",
    "        \n",
    "        sum_acc += acc\n",
    "    \n",
    "    cur_acc = sum_acc / folds\n",
    "    \n",
    "    print(\"reg_coeff: {}, acc: {:.3f}\".format(1.0/reg, cur_acc))\n",
    "    \n",
    "    # store the best hyperparameter\n",
    "    if cur_acc > best_acc:\n",
    "        best_acc = cur_acc\n",
    "        best_reg = reg\n",
    "        \n",
    "print('best reg coeff:', 1.0/best_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Evaluate the learned model\n",
    "\n",
    "After getting the best hyperparameter $\\lambda$, we retrain the model with the train_val set. Then, we evaluate this  model on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.000, recall: 1.000, precision: 1.000, f1: 1.000,\n"
     ]
    }
   ],
   "source": [
    "# retrain the model\n",
    "clf = LogisticRegression(penalty='l2', C=best_reg, solver='lbfgs')\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(\"accuracy: {:.3f}, recall: {:.3f}, precision: {:.3f}, f1: {:.3f},\".format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task\n",
    "\n",
    "Here, we use the [heart disease](./heart.csv) dataset. Each sample has the following feature: \n",
    "\n",
    "* age\n",
    "* sex\n",
    "* chest pain type (4 values)\n",
    "* resting blood pressure\n",
    "* serum cholestoral in mg/dl\n",
    "* fasting blood sugar > 120 mg/dl\n",
    "* resting electrocardiographic results (values 0,1,2)\n",
    "* maximum heart rate achieved\n",
    "* exercise induced angina\n",
    "* oldpeak = ST depression induced by exercise relative to rest\n",
    "* the slope of the peak exercise ST segment\n",
    "* number of major vessels (0-3) colored by flourosopy\n",
    "* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "The last column refers to the presence of heart disease in the patient.\n",
    "\n",
    "The task is to predict whether a person has the heart disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocess the raw data\n",
    "\n",
    "* Check whether there are missing values\n",
    "* Check whether theare are cateogrical features\n",
    "* Check whether this dataset is balanced or not (use the bar plot to visualize the number of positive and negative samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Split the data into two subsets and normalize the features of samples\n",
    "\n",
    "* Split the dataset into the train_val set and testing set. \n",
    "* Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the logistic regression model and select the hyperparameter with cross-validation\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\min_{\\mathbf{w}} \\sum_{i=1}^{n}\\{\\log(1+\\exp(\\mathbf{w}^T\\mathbf{x}_i))-y_i\\mathbf{w}^T\\mathbf{x}_i \\} + \\lambda\\|\\mathbf{w}\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "* Use the 10-fold cross-validation to select the hyperparameter $\\lambda$.\n",
    "* Search $\\lambda$ from $\\{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 20, 50, 100\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate the learned model\n",
    "\n",
    "* Report the prediction accuracy, recall, precision, and F1 score.\n",
    "\n",
    "* Use the bar plot to visulaize the elements of the learned model parameter vector $\\mathbf{w}$. Some elements  have larger absolute values, while the others do not. Try to explain this phenomenon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
